# ğŸ›¡ï¸ BULLETPROOF ASR TRAINING - TROUBLESHOOTING GUIDE

## Common Issues & Auto-Fixes

### ğŸ”„ Google Colab Auto-Reload / Disconnection

**Problem:** Colab disconnects or reloads automatically during training.

**Auto-Fix Applied:**  
âœ… JavaScript keep-alive widget injected  
âœ… Connection monitoring active  
âœ… Auto-checkpoint resume enabled

**Manual Steps (if needed):**
1. Keep your browser tab open (don't close it)
2. Open browser console (F12) to see keep-alive pings
3. If disconnected, simply re-run the training cell - it will auto-resume!

---

### ğŸ’¥ Out of Memory (OOM) Errors

**Problem:** `RuntimeError: CUDA out of memory`

**Auto-Fix Applied:**  
âœ… Automatic batch size reduction  
âœ… CUDA cache clearing  
âœ… Retry with smaller batch (up to 3 attempts)

**If still failing:**
```python
# Reduce safety margin
python src/cli.py train --safety_margin=0.70
```

---

### ğŸŒ Network Timeout / Connection Errors

**Problem:** Dataset download fails or HuggingFace API timeouts

**Auto-Fix Applied:**  
âœ… Automatic retry (up to 3 attempts)  
âœ… 10-second backoff between retries  
âœ… Network connectivity checks

**Manual Check:**
```bash
# Test internet
ping google.com

# Test HuggingFace
curl https://huggingface.co
```

---

### ğŸ’¾ Disk Full Errors

**Problem:** `IOError: No space left on device`

**Auto-Detection:**  
âœ… Pre-flight disk space check  
âœ… Recommended cleanup shown

**Manual Fix:**
```bash
# Clear HuggingFace cache
rm -rf ~/.cache/huggingface

# Check space
df -h

# Cleanup old checkpoints
rm -rf outputs/checkpoint-old*
```

---

### ğŸ” HuggingFace Authentication Issues

**Problem:** Cannot access dataset or push model

**Auto-Detection:**  
âœ… Pre-flight token check

**Manual Fix:**
```python
from huggingface_hub import login
login()  # Paste your token
```

Or via CLI:
```bash
huggingface-cli login
```

---

### âš¡ GPU Not Detected

**Problem:** Training runs on CPU (very slow)

**Auto-Detection:**  
âœ… Pre-flight GPU check  
âœ… Warning shown if CPU-only

**Manual Fix (Colab):**
1. Runtime â†’ Change runtime type
2. Select "GPU" (T4, L4, or A100)
3. Click "Save"
4. Re-run training

---

### ğŸ”„ Training Interrupted (Ctrl+C)

**Auto-Recovery:**  
âœ… Graceful shutdown triggered  
âœ… Emergency checkpoint saved  
âœ… Can resume from checkpoint

**To Resume:**
```bash
# Auto-detects latest checkpoint
python src/cli.py train
```

---

### ğŸ“¦ Missing Dependencies

**Problem:** `ModuleNotFoundError`

**Auto-Detection:**  
âœ… Pre-flight dependency check  
âœ… Shows missing packages

**Manual Fix:**
```bash
pip install -r requirements.txt
```

---

## ğŸš¨ Error Codes

| Error | Meaning | Auto-Fix |
|-------|---------|----------|
| OOM | Out of memory | âœ… Yes - reduces batch size |
| CUDA Error | GPU issue | âœ… Yes - resets CUDA context |
| ConnectionError | Network down | âœ… Yes - retries with backoff |
| IOError (disk) | Disk full | âš ï¸ Partial - shows cleanup tips |
| Import Error | Missing package | âŒ No - install manually |

---

## ğŸ“‹ Pre-Flight Checklist

Before training, the system automatically checks:

- [ ] âœ… GPU available
- [ ] âœ… Sufficient RAM (8GB+)
- [ ] âœ… Sufficient disk space (50GB+)
- [ ] âœ… All dependencies installed
- [ ] âœ… HuggingFace token valid
- [ ] âœ… Internet connection
- [ ] âœ… Write permissions

**If any fail:** Fix the issue and try again!

---

## ğŸ”§ Recovery Features

### Automatic
- âœ… Checkpoint resume after disconnection
- âœ… Retry on transient errors (3x)
- âœ… Memory cleanup on OOM
- âœ… Graceful shutdown on Ctrl+C

### Manual
- Run health check: `python src/health_check.py`
- View keep-alive status: Check browser console (F12)
- Find checkpoints: `ls outputs/checkpoint-*`

---

## ğŸ’¡ Pro Tips

1. **For long training on Colab:**
   - Use Colab Pro for longer sessions
   - Keep browser tab open
   - Check console for keep-alive pings

2. **For unstable networks:**
   - System auto-retries 3 times
   - Use `--resume_from_checkpoint` if manual resume needed

3. **For memory issues:**
   - Start with `--safety_margin=0.70` (conservative)
   - Use smaller model if needed
   - Enable `--use_deepspeed=True` for 2x memory savings

4. **For debugging:**
   - Check `training_config.json` for actual settings used
   - View TensorBoard: `tensorboard --logdir outputs/`
   - Check logs: `tail -f training.log`

---

## ğŸ“ Still Having Issues?

1. **Run full health check:**
   ```bash
   python src/health_check.py
   ```

2. **Check this guide:** Go through each section above

3. **Enable verbose logging:**
   ```bash
   export TRANSFORMERS_VERBOSITY=debug
   python src/cli.py train
   ```

---

**Remember:** The system is designed to handle 99% of issues automatically. If training fails, just re-run - it will resume from the last checkpoint! ğŸš€
