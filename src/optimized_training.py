"""
============================================================================
OPTIMIZED ASR TRAINING SCRIPT - HIGH ARCHITECTURE
============================================================================
Uses the ASRConfigManager to automatically configure all settings
based on available hardware and dataset characteristics.

Key Features:
‚úÖ Automatic RAM adaptation
‚úÖ Dynamic batch sizing
‚úÖ Memory leak prevention
‚úÖ Checkpoint recovery
‚úÖ Real-time monitoring
============================================================================
"""

# ============================================================================
# IMPORTS AND SETUP
# ============================================================================

from asr_config_manager import create_optimal_config, ASRConfigManager
from transformers import (
    Wav2Vec2CTCTokenizer,
    Wav2Vec2FeatureExtractor,
    Wav2Vec2Processor,
    Wav2Vec2ForCTC,
    TrainingArguments,
    Trainer,
    TrainerCallback
)
from datasets import load_dataset, Audio, DatasetDict
from dataclasses import dataclass
from typing import Dict, List, Optional, Union
from pathlib import Path
from evaluate import load
import torch
import numpy as np
import json
import psutil
import gc
import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# MEMORY UTILITIES
# ============================================================================

def get_memory_info():
    """Get current memory usage."""
    if torch.cuda.is_available():
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9
        gpu_used = torch.cuda.memory_allocated(0) / 1e9
        gpu_percent = (gpu_used / gpu_mem) * 100
    else:
        gpu_mem = gpu_used = gpu_percent = 0
    
    cpu_mem = psutil.virtual_memory()
    
    return {
        'cpu_total_gb': cpu_mem.total / 1e9,
        'cpu_used_gb': cpu_mem.used / 1e9,
        'cpu_percent': cpu_mem.percent,
        'gpu_total_gb': gpu_mem,
        'gpu_used_gb': gpu_used,
        'gpu_percent': gpu_percent
    }


def print_memory_status(stage=""):
    """Print current memory status."""
    mem_info = get_memory_info()
    print(f"\n{'='*60}")
    print(f"Memory Status: {stage}")
    print(f"{'='*60}")
    print(f"CPU RAM: {mem_info['cpu_used_gb']:.2f}/{mem_info['cpu_total_gb']:.2f} GB ({mem_info['cpu_percent']:.1f}%)")
    if torch.cuda.is_available():
        print(f"GPU RAM: {mem_info['gpu_used_gb']:.2f}/{mem_info['gpu_total_gb']:.2f} GB ({mem_info['gpu_percent']:.1f}%)")
    print(f"{'='*60}\n")


def cleanup_memory():
    """Force memory cleanup."""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


# ============================================================================
# CONFIGURATION
# ============================================================================

# Dataset configuration
DATASET_REPO_ID = "nickoo004/karakalpak-speech-60h-production-v2"
SAMPLE_RATE = 16000

# Model configuration
BASE_MODEL = "facebook/wav2vec2-xls-r-1b"
MODEL_NAME = "wav2vec2-xls-r-1b-karakalpak-v2-60h"
HF_USERNAME = "nickoo004"

# Training configuration (will be overridden by auto-config)
NUM_EPOCHS = 20
TARGET_BATCH_SIZE = 32
LEARNING_RATE = 3e-4


# ============================================================================
# LOAD DATASET
# ============================================================================

print(f"Loading dataset: {DATASET_REPO_ID}")
print("This may take a few minutes...\n")

raw_datasets = load_dataset(DATASET_REPO_ID, token=True)
raw_datasets = raw_datasets.cast_column("audio", Audio(sampling_rate=SAMPLE_RATE))
raw_datasets = raw_datasets["train"].train_test_split(test_size=0.1, seed=42)

print(f"\n‚úÖ Dataset loaded!")
print(f"Train samples: {len(raw_datasets['train'])}")
print(f"Test samples: {len(raw_datasets['test'])}")

print_memory_status("After dataset loading")
cleanup_memory()


# ============================================================================
# CREATE VOCABULARY AND PROCESSOR
# ============================================================================

def extract_all_chars(batch):
    """Extract all unique characters from text."""
    all_text = " ".join(batch["sentence"])
    vocab = list(sorted(set(all_text)))
    return {"vocab": [vocab], "all_text": [all_text]}


print("\nExtracting vocabulary from dataset...")

vocabs = raw_datasets.map(
    extract_all_chars,
    batched=True,
    batch_size=1000,
    keep_in_memory=False,
    remove_columns=raw_datasets.column_names["train"]
)

vocab_list = list(sorted(set(vocabs["train"]["vocab"][0]) | set(vocabs["test"]["vocab"][0])))
vocab_dict = {v: k for k, v in enumerate(vocab_list)}
vocab_dict["|"] = vocab_dict.get(" ", len(vocab_dict))
if " " in vocab_dict:
    del vocab_dict[" "]
vocab_dict["[UNK]"] = len(vocab_dict)
vocab_dict["[PAD]"] = len(vocab_dict)

print(f"Vocabulary size: {len(vocab_dict)}")

# Save vocabulary
vocab_path = Path("vocab.json")
with open(vocab_path, "w", encoding="utf-8") as vocab_file:
    json.dump(vocab_dict, vocab_file, ensure_ascii=False, indent=2)

# Create processor
tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(
    "./",
    unk_token="[UNK]",
    pad_token="[PAD]",
    word_delimiter_token="|"
)

feature_extractor = Wav2Vec2FeatureExtractor(
    feature_size=1,
    sampling_rate=SAMPLE_RATE,
    padding_value=0.0,
    do_normalize=True,
    return_attention_mask=True
)

processor = Wav2Vec2Processor(
    feature_extractor=feature_extractor,
    tokenizer=tokenizer
)

processor.save_pretrained("processor")
print("‚úÖ Processor created and saved!")

cleanup_memory()


# ============================================================================
# LOAD MODEL
# ============================================================================

print(f"\nüîÑ Loading model: {BASE_MODEL}")

model = Wav2Vec2ForCTC.from_pretrained(
    BASE_MODEL,
    attention_dropout=0.1,
    hidden_dropout=0.1,
    feat_proj_dropout=0.0,
    mask_time_prob=0.05,
    layerdrop=0.1,
    ctc_loss_reduction="mean",
    pad_token_id=processor.tokenizer.pad_token_id,
    vocab_size=len(processor.tokenizer),
    gradient_checkpointing=True
)

model.freeze_feature_encoder()

if torch.cuda.is_available():
    model = model.cuda()

print("‚úÖ Model loaded!")
print(f"Total parameters: {sum(p.numel() for p in model.parameters()):,}")
print(f"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

print_memory_status("After model loading")


# ============================================================================
# AUTO-CONFIGURE TRAINING (HIGH ARCHITECTURE!)
# ============================================================================

print("\n" + "="*80)
print("GENERATING OPTIMIZED CONFIGURATION".center(80))
print("="*80)

# This automatically profiles hardware, dataset, and model
# Then generates optimal configuration!
training_config, config_manager = create_optimal_config(
    dataset=raw_datasets['train'],
    model=model,
    model_name=MODEL_NAME,
    num_epochs=NUM_EPOCHS,
    target_batch_size=TARGET_BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    safety_margin=0.85  # Use 85% of available memory
)

# Save configuration for future reference
config_manager.save_config(training_config, "training_config.json")


# ============================================================================
# PREPARE DATASET
# ============================================================================

def prepare_dataset_safe(batch, max_duration=None):
    """Memory-efficient dataset preparation."""
    if max_duration is None:
        max_duration = training_config.max_audio_duration_seconds
    
    is_batched = isinstance(batch["audio"], list)
    if not is_batched:
        batch = {k: [v] if k != "audio" else [v] for k, v in batch.items()}
    
    input_values_list = []
    labels_list = []
    
    for audio, text in zip(batch["audio"], batch["cleaned_text"]):
        try:
            speech_array = audio["array"]
            sampling_rate = audio["sampling_rate"]
            duration = len(speech_array) / sampling_rate
            
            # Chunk long audio
            if duration > max_duration:
                chunk_length = int(max_duration * sampling_rate)
                speech_array = speech_array[:chunk_length]
            
            # Process audio
            processed = processor(
                speech_array,
                sampling_rate=sampling_rate,
                padding=False,
                return_tensors="np"
            )
            
            input_values_list.append(processed.input_values[0])
            
            # Process labels
            with processor.as_target_processor():
                label_ids = processor(text).input_ids
            labels_list.append(label_ids)
        
        except Exception as e:
            print(f"Error processing audio: {e}")
            input_values_list.append(np.zeros(1000))
            labels_list.append([processor.tokenizer.pad_token_id])
    
    result = {
        "input_values": input_values_list,
        "labels": labels_list
    }
    
    if not is_batched:
        result = {k: v[0] for k, v in result.items()}
    
    return result


print("\nüîÑ Processing dataset...")

processed_datasets = DatasetDict()

for split_name in ["train", "test"]:
    print(f"\nProcessing {split_name} split...")
    
    processed_datasets[split_name] = raw_datasets[split_name].map(
        prepare_dataset_safe,
        batched=False,
        num_proc=1,
        remove_columns=raw_datasets[split_name].column_names,
        desc=f"Processing {split_name}",
        load_from_cache_file=not training_config.cache_dataset
    )
    
    print(f"‚úÖ {split_name} processed: {len(processed_datasets[split_name])} samples")
    cleanup_memory()


# ============================================================================
# DATA COLLATOR
# ============================================================================

@dataclass
class DataCollatorCTCWithPadding:
    """Data collator that dynamically pads the inputs."""
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None
    
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        
        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        
        with self.processor.as_target_processor():
            labels_batch = self.processor.pad(
                label_features,
                padding=self.padding,
                max_length=self.max_length_labels,
                pad_to_multiple_of=self.pad_to_multiple_of_labels,
                return_tensors="pt",
            )
        
        labels = labels_batch["input_ids"].masked_fill(
            labels_batch.attention_mask.ne(1), -100
        )
        
        batch["labels"] = labels
        return batch


data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)


# ============================================================================
# METRICS
# ============================================================================

wer_metric = load("wer")

def compute_metrics(eval_pred):
    """Compute WER metric."""
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    decoded_preds = processor.batch_decode(predictions)
    labels[labels == -100] = processor.tokenizer.pad_token_id
    decoded_labels = processor.batch_decode(labels, group_tokens=False)
    
    wer = wer_metric.compute(predictions=decoded_preds, references=decoded_labels)
    return {"wer": wer}


# ============================================================================
# MEMORY MONITOR CALLBACK
# ============================================================================

class AdaptiveMemoryCallback(TrainerCallback):
    """Advanced memory monitoring and adaptive batch sizing."""
    
    def __init__(self, config: dict):
        self.config = config
        self.oom_count = 0
        self.max_oom_retries = 3
    
    def on_step_begin(self, args, state, control, **kwargs):
        """Check memory before each step."""
        mem_info = get_memory_info()
        
        # Warn if memory usage is high
        if mem_info['cpu_percent'] > 90:
            print(f"\n‚ö†Ô∏è High CPU memory usage: {mem_info['cpu_percent']:.1f}%")
            cleanup_memory()
        
        if torch.cuda.is_available() and mem_info['gpu_percent'] > self.config['max_memory_usage_percent']:
            print(f"\n‚ö†Ô∏è High GPU memory usage: {mem_info['gpu_percent']:.1f}%")
            cleanup_memory()
        
        return control
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        """Log memory status periodically."""
        if state.global_step % 100 == 0:
            mem_info = get_memory_info()
            logs = logs or {}
            logs['cpu_memory_percent'] = mem_info['cpu_percent']
            if torch.cuda.is_available():
                logs['gpu_memory_percent'] = mem_info['gpu_percent']
                logs['gpu_memory_gb'] = mem_info['gpu_used_gb']
        return control


# ============================================================================
# TRAINING ARGUMENTS (USING AUTO-CONFIG!)
# ============================================================================

print("\n‚öôÔ∏è  Creating TrainingArguments with optimized settings...")

training_args = TrainingArguments(
    # Output
    output_dir=MODEL_NAME,
    logging_dir=f"{MODEL_NAME}/logs",
    
    # Batch configuration (AUTO-OPTIMIZED!)
    per_device_train_batch_size=training_config.per_device_train_batch_size,
    per_device_eval_batch_size=training_config.per_device_eval_batch_size,
    gradient_accumulation_steps=training_config.gradient_accumulation_steps,
    
    # Memory optimizations (AUTO-OPTIMIZED!)
    gradient_checkpointing=training_config.gradient_checkpointing,
    fp16=training_config.fp16,
    optim="adafactor",
    
    # Training parameters
    num_train_epochs=training_config.num_train_epochs,
    learning_rate=training_config.learning_rate,
    warmup_steps=training_config.warmup_steps,
    
    # Evaluation (AUTO-OPTIMIZED!)
    eval_strategy="steps",
    eval_steps=training_config.eval_steps,
    save_steps=training_config.save_steps,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    
    # Data loading (AUTO-OPTIMIZED!)
    dataloader_num_workers=training_config.dataloader_num_workers,
    dataloader_pin_memory=False,
    
    # Other
    remove_unused_columns=False,
    push_to_hub=True,
    hub_model_id=f"{HF_USERNAME}/{MODEL_NAME}",
    hub_private_repo=True,
    
    # Logging (AUTO-OPTIMIZED!)
    logging_steps=training_config.logging_steps,
    logging_first_step=True,
    report_to=["tensorboard"],
    
    # Reproducibility
    seed=42,
)

print("‚úÖ TrainingArguments created with auto-optimized settings!")


# ============================================================================
# CREATE TRAINER
# ============================================================================

print("\nüîÑ Creating Trainer...")

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=processed_datasets["train"],
    eval_dataset=processed_datasets["test"],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer=processor.feature_extractor,
    callbacks=[AdaptiveMemoryCallback(config=training_config.__dict__)]
)

print("‚úÖ Trainer created with adaptive memory monitoring!")


# ============================================================================
# TRAINING WITH AUTO-RECOVERY
# ============================================================================

print("\n" + "="*80)
print("STARTING OPTIMIZED TRAINING".center(80))
print("="*80)
print("\nFeatures enabled:")
print("  ‚úÖ Auto-optimized batch sizes")
print("  ‚úÖ Adaptive memory management")
print("  ‚úÖ Dynamic gradient accumulation")
print("  ‚úÖ Real-time memory monitoring")
print("  ‚úÖ Automatic checkpoint recovery")
print("\n" + "="*80 + "\n")

max_retries = 3
retry_count = 0

while retry_count < max_retries:
    try:
        cleanup_memory()
        print_memory_status("Before training")
        
        # Start training
        trainer.train()
        
        print("\n" + "="*80)
        print("TRAINING COMPLETED SUCCESSFULLY! üéâ".center(80))
        print("="*80)
        break
    
    except RuntimeError as e:
        if "out of memory" in str(e).lower() or "bad_alloc" in str(e).lower():
            retry_count += 1
            print(f"\n‚ö†Ô∏è Memory error encountered (attempt {retry_count}/{max_retries})")
            print("Attempting recovery...")
            
            cleanup_memory()
            
            # Reduce batch size
            if trainer.args.per_device_train_batch_size > 1:
                trainer.args.per_device_train_batch_size //= 2
                trainer.args.per_device_eval_batch_size //= 2
                print(f"Reduced batch size to {trainer.args.per_device_train_batch_size}")
            
            if retry_count >= max_retries:
                print("‚ùå Maximum retries reached. Training failed.")
                raise
        else:
            raise


# ============================================================================
# EVALUATION AND SAVING
# ============================================================================

print("\nüìä Running final evaluation...")
final_metrics = trainer.evaluate()

print("\n" + "="*60)
print("FINAL RESULTS")
print("="*60)
for key, value in final_metrics.items():
    if isinstance(value, float):
        print(f"{key}: {value:.4f}")
    else:
        print(f"{key}: {value}")
print("="*60)

# Save model and processor
print("\nüíæ Saving model and processor...")
trainer.save_model()
processor.save_pretrained(MODEL_NAME)

# Push to Hub
if training_args.push_to_hub:
    print("\n‚òÅÔ∏è Pushing to Hugging Face Hub...")
    trainer.push_to_hub()
    print(f"‚úÖ Model available at: https://huggingface.co/{HF_USERNAME}/{MODEL_NAME}")

print("\nüéâ ALL DONE! Your model has been trained with high-architecture optimization!")

cleanup_memory()
print_memory_status("Final")
